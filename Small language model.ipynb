{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "H100",
   "provenance": []
  },
  "lightning_ai": {
   "gpuType": "H200",
   "accelerator": "gpu",
   "machineType": "H200"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Ultimate GPT-Vibe Training Pipeline - 1.3B (H200 OPTIMIZED)\n## \"Early GPT-vibe assistant, *before* RAG, good behavior, low derp.\"\n\n---\n\n## H200 OPTIMIZATIONS APPLIED:\n1. **H200 GPU Detection & Configuration** - Full support for 80GB/141GB HBM3\n2. **BF16 Mixed Precision** - Optimal for Hopper architecture (better than FP16)\n3. **Flash Attention 2** - Native Hopper support for efficient attention\n4. **Larger Batch Sizes** - Leverage 3.35 TB/s memory bandwidth\n5. **Extended Sequence Lengths** - Up to 4096 tokens per sequence\n6. **CUDA 12.x Support** - Latest optimizations for Hopper GPUs\n7. **Lightning AI Integration** - Optimized for Lightning AI platform\n8. **Fewer Shards** - H200 can handle larger data chunks efficiently\n\n---\n\n## Goal\nBuild a **1.3B parameter** decoder-only transformer (pilot) that:\n- Has good general assistant behavior\n- Provides reasonable banking Q&A\n- Can say \"I don't know / check official source\" instead of hallucinating\n- Feels like a real assistant even **before RAG**\n\n---\n\n## Training Curriculum Overview\n\n| Phase | Name | Data Type | Learning Rate | Purpose |\n|-------|------|-----------|---------------|--------|\n| **B** | Pretraining | Raw text (~10-20B tokens pilot) | 2e-4 | Brain formation |\n| **C** | Generic SFT | Instruction datasets + **Safety** | 5e-6 | Assistant behavior |\n| **D** | Banking SFT | Domain + Generic mix | 1e-6 | Industry specialization |\n\n---\n\n## H200 vs A100 Performance Comparison\n\n| Metric | A100 80GB | H200 80GB | H200 141GB |\n|--------|-----------|-----------|------------|\n| Memory | 80GB HBM2e | 80GB HBM3 | 141GB HBM3e |\n| Bandwidth | 2.0 TB/s | 3.35 TB/s | 4.8 TB/s |\n| FP16 TFLOPs | 312 | 989 | 989 |\n| BF16 TFLOPs | 312 | 989 | 989 |\n| FP8 TFLOPs | N/A | 1979 | 1979 |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# MASTER TASK LIST: 1.3B Pilot (H200 Edition)\n\n## H200 Optimizations Applied:\n- [x] H200 GPU tier detection and configuration\n- [x] BF16 precision instead of FP16 for Hopper\n- [x] Flash Attention 2 support\n- [x] CUDA 12.x compatibility\n- [x] Optimized batch sizes for 80GB+ HBM3\n- [x] Lightning AI platform integration\n\n## Token Calculation:\n```\ntokens = steps x batch_size x max_seq_len\n```\n\n## Pilot Step Summary (H200):\n| Phase | Steps | Batch | Seq Len | Est. Tokens |\n|-------|-------|-------|---------|-------------|\n| Pretraining | 100k | 64 | 4096 | ~26B |\n| SFT + Safety | 120k | 32 | 2048 | ~8B |\n| Domain | 15k | 32 | 2048 | ~1B |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 1: Install Dependencies (CUDA 12.x for H200/Hopper)\n# H200 requires CUDA 12.x for optimal performance\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -q\n!pip install transformers datasets tokenizers accelerate einops matplotlib numpy tqdm tiktoken -q\n!pip install flash-attn --no-build-isolation -q 2>/dev/null || echo \"Flash Attention will use PyTorch SDPA fallback\"\nprint(\"\\n\" + \"=\"*60)\nprint(\"Dependencies installed (CUDA 12.1 for H200/Hopper)\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 2: Core Imports and Setup (H200 Optimized)\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, IterableDataset\nfrom torch.amp import GradScaler, autocast  # Updated for PyTorch 2.x\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport json\nimport gc\nimport math\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Tuple, List, Dict, Callable\nimport warnings\nfrom datetime import datetime\nfrom datasets import load_dataset\nimport tiktoken\nfrom itertools import islice\nimport shutil\nimport glob\nimport time\nwarnings.filterwarnings('ignore')\n\n# H200/Hopper-specific CUDA settings\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nos.environ['CUDA_DEVICE_MAX_CONNECTIONS'] = '1'  # Better for large models\n\nprint(\"=\"*80)\nprint(\"ULTIMATE GPT-VIBE TRAINING PIPELINE - 1.3B (H200 OPTIMIZED)\")\nprint(\"Phase B: Pretraining | Phase C: SFT + Safety | Phase D: Domain FT\")\nprint(\"Optimized for: NVIDIA H200 GPU with HBM3 Memory\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 3: H200 GPU Detection and Advanced Optimization\ndef check_gpu():\n    \"\"\"Detect GPU type with full H200/Hopper support and apply optimal settings\"\"\"\n    if torch.cuda.is_available():\n        num_gpus = torch.cuda.device_count()\n        gpu_name = torch.cuda.get_device_name(0)\n        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n        \n        # Get compute capability for architecture detection\n        major, minor = torch.cuda.get_device_capability(0)\n        compute_capability = f\"{major}.{minor}\"\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"GPU DETECTION RESULTS\")\n        print(f\"{'='*60}\")\n        print(f\"GPU Model: {gpu_name}\")\n        print(f\"Total Memory: {total_memory:.2f} GB\")\n        print(f\"Number of GPUs: {num_gpus}\")\n        print(f\"Compute Capability: {compute_capability}\")\n        print(f\"CUDA Version: {torch.version.cuda}\")\n        print(f\"PyTorch Version: {torch.__version__}\")\n\n        # Memory allocation settings\n        torch.cuda.set_per_process_memory_fraction(0.95)\n        \n        # Enable TF32 for Ampere+ (A100, H100, H200)\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n        torch.backends.cudnn.benchmark = True\n        torch.backends.cudnn.deterministic = False\n        \n        # Check for Hopper architecture (compute capability 9.0)\n        is_hopper = major >= 9\n        \n        # Enable Flash Attention / Memory Efficient Attention\n        flash_available = False\n        try:\n            torch.backends.cuda.enable_flash_sdp(True)\n            torch.backends.cuda.enable_mem_efficient_sdp(True)\n            torch.backends.cuda.enable_math_sdp(False)  # Disable slow fallback\n            flash_available = True\n            print(f\"Flash/Memory-Efficient Attention: ENABLED\")\n        except Exception as e:\n            print(f\"Flash Attention: Fallback mode ({e})\")\n        \n        # Detect GPU tier with H200 support\n        if 'H200' in gpu_name:\n            if total_memory > 120:\n                gpu_tier = 'H200_141GB'\n            else:\n                gpu_tier = 'H200_80GB'\n            print(f\"\\n*** H200 DETECTED - Using Maximum Performance Settings ***\")\n        elif 'H100' in gpu_name:\n            if total_memory > 70:\n                gpu_tier = 'H100_80GB'\n            else:\n                gpu_tier = 'H100_40GB'\n        elif 'A100' in gpu_name:\n            if total_memory > 70:\n                gpu_tier = 'A100_80GB'\n            else:\n                gpu_tier = 'A100_40GB'\n        elif 'L40' in gpu_name:\n            gpu_tier = 'L40'\n        elif 'L4' in gpu_name:\n            gpu_tier = 'L4'\n        elif 'T4' in gpu_name:\n            gpu_tier = 'T4'\n            print(\"   NOTE: 1.3B on T4 is tight. May need to reduce batch/seq_len if OOM.\")\n        elif 'V100' in gpu_name:\n            gpu_tier = 'V100'\n        elif is_hopper:\n            # Generic Hopper detection (catches H200 variants)\n            gpu_tier = 'H200_80GB' if total_memory > 70 else 'H100_40GB'\n            print(f\"   Detected Hopper architecture GPU\")\n        else:\n            # Fallback based on memory\n            if total_memory > 70:\n                gpu_tier = 'A100_80GB'\n            elif total_memory > 35:\n                gpu_tier = 'A100_40GB'\n            else:\n                gpu_tier = 'T4'\n        \n        # H200-specific optimizations\n        if 'H200' in gpu_tier or 'H100' in gpu_tier:\n            # Enable BF16 (better for Hopper than FP16)\n            print(f\"BF16 Precision: ENABLED (optimal for Hopper)\")\n            \n            # Enable cuDNN autotuning\n            torch.backends.cudnn.benchmark = True\n            \n            # Try to enable torch.compile for extra speed\n            try:\n                if hasattr(torch, 'compile'):\n                    print(f\"torch.compile: AVAILABLE\")\n            except:\n                pass\n        \n        print(f\"\\nSelected GPU Tier: {gpu_tier}\")\n        print(f\"{'='*60}\\n\")\n        return gpu_tier, num_gpus, total_memory, is_hopper\n    else:\n        print(\"No GPU detected! This training requires a GPU.\")\n        return 'CPU', 0, 0, False\n\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\ngpu_tier, num_gpus, gpu_memory, is_hopper = check_gpu()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 4: Disk Management\ndef get_dir_size(path):\n    total = 0\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for filename in filenames:\n                filepath = os.path.join(dirpath, filename)\n                if os.path.exists(filepath):\n                    total += os.path.getsize(filepath)\n    except:\n        pass\n    return total\n\ndef clear_huggingface_cache():\n    clear_memory()\n    print(\"\\nClearing HuggingFace cache...\")\n    cache_dir = os.path.expanduser(\"~/.cache/huggingface\")\n    datasets_cache = os.path.join(cache_dir, \"datasets\")\n    downloads_cache = os.path.join(cache_dir, \"downloads\")\n    space_freed = 0\n    for cache_path in [datasets_cache, downloads_cache]:\n        if os.path.exists(cache_path):\n            try:\n                size = get_dir_size(cache_path)\n                shutil.rmtree(cache_path)\n                os.makedirs(cache_path, exist_ok=True)\n                space_freed += size\n                print(f\"   Cleared {os.path.basename(cache_path)} ({size/1e9:.2f} GB)\")\n            except Exception as e:\n                print(f\"   Error: {e}\")\n    print(f\"   Total freed: {space_freed/1e9:.2f} GB\")\n    return space_freed\n\ndef print_disk_status():\n    total, used, free = shutil.disk_usage(\"/\")\n    print(f\"\\nDisk: {used/1e9:.1f}GB used / {total/1e9:.1f}GB total | Free: {free/1e9:.1f} GB\")\n\nprint_disk_status()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 5: HuggingFace Authentication\nfrom huggingface_hub import login\n\nHF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\nif HF_TOKEN is None:\n    HF_TOKEN = \"\"  # Paste token here if needed\n\nif HF_TOKEN:\n    login(token=HF_TOKEN)\n    print(\"Logged in to HuggingFace Hub\")\nelse:\n    print(\"No HuggingFace token. Most datasets work without it.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 6: GPU-Optimized Phase Configurations (H200 ADDED!)\n# Massive optimizations for H200's 80GB+ HBM3 memory and 3.35+ TB/s bandwidth\n\ndef get_phase_configs(gpu_tier):\n    configs = {\n        # T4 - 16GB (Minimum viable for 1.3B)\n        'T4': {\n            'phase_b': {'max_seq_len': 512, 'batch_size': 4, 'gradient_accumulation_steps': 32, 'learning_rate': 2e-4, 'warmup_steps': 1000, 'weight_decay': 0.1, 'min_lr_factor': 0.1, 'buffer_size': 10000},\n            'phase_c': {'max_seq_len': 512, 'batch_size': 2, 'gradient_accumulation_steps': 16, 'learning_rate': 5e-6, 'warmup_steps': 300, 'weight_decay': 0.01, 'min_lr_factor': 0.1, 'buffer_size': 5000},\n            'phase_d': {'max_seq_len': 512, 'batch_size': 2, 'gradient_accumulation_steps': 8, 'learning_rate': 1e-6, 'warmup_steps': 100, 'weight_decay': 0.01, 'min_lr_factor': 0.1, 'buffer_size': 2000},\n        },\n        # L4 - 24GB\n        'L4': {\n            'phase_b': {'max_seq_len': 1024, 'batch_size': 8, 'gradient_accumulation_steps': 16, 'learning_rate': 2e-4, 'warmup_steps': 1500, 'weight_decay': 0.1, 'min_lr_factor': 0.1, 'buffer_size': 20000},\n            'phase_c': {'max_seq_len': 1024, 'batch_size': 4, 'gradient_accumulation_steps': 8, 'learning_rate': 5e-6, 'warmup_steps': 400, 'weight_decay': 0.01, 'min_lr_factor': 0.1, 'buffer_size': 10000},\n            'phase_d': {'max_seq_len': 1024, 'batch_size': 4, 'gradient_accumulation_steps': 4, 'learning_rate': 1e-6, 'warmup_steps': 150, 'weight_decay': 0.01, 'min_lr_factor': 0.1, 'buffer_size': 5000},\n        },\n        # A100 40GB\n        'A100_40GB': {\n            'phase_b': {'max_seq_len': 2048, 'batch_size': 32, 'gradient_accumulation_steps': 4, 'learning_rate': 2e-4, 'warmup_steps': 2000, 'weight_decay': 0.1, 'min_lr_factor': 0.1, 'buffer_size': 50000},\n            'phase_c': {'max_seq_len': 2048, 'batch_size': 16, 'gradient_accumulation_steps': 2, 'learning_rate': 5e-6, 'warmup_steps': 500, 'weight_decay': 0.01, 'min_lr_factor': 0.1, 'buffer_size': 20000},\n            'phase_d': {'max_seq_len': 2048, 'batch_size': 16, 'gradient_accumulation_steps': 1, 'learning_rate': 1e-6, 'warmup_steps': 200, 'weight_decay': 0.01, 'min_lr_factor': 0.1, 'buffer_size': 10000},\n        },\n        # A100 80GB\n        'A100_80GB': {\n            'phase_b': {'max_seq_len': 2048, 'batch_size': 48, 'gradient_accumulation_steps': 2, 'learning_rate': 2e-4, 'warmup_steps': 2000, 'weight_decay': 0.1, 'min_lr_factor': 0.1, 'buffer_size': 100000},\n            'phase_c': {'max_seq_len': 2048, 'batch_size': 24, 'gradient_accumulation_steps': 1, 'learning_rate': 5e-6, 'warmup_steps': 500, 'weight_decay': 0.01, 'min_lr_factor': 0.1, 'buffer_size': 50000},\n            'phase_d': {'max_seq_len': 2048, 'batch_size': 24, 'gradient_accumulation_steps': 1, 'learning_rate': 1e-6, 'warmup_steps': 200, 'weight_decay': 0.01, 'min_lr_factor': 0.1, 'buffer_size': 20000},\n        },\n        # H100 80GB (Hopper)\n        'H100_80GB': {\n            'phase_b': {'max_seq_len': 4096, 'batch_size': 56, 'gradient_accumulation_steps': 2, 'learning_rate': 2e-4, 'warmup_steps': 2500, 'weight_decay': 0.1, 'min_lr_factor': 0.1, 'buffer_size': 150000},\n            'phase_c': {'max_seq_len': 2048, 'batch_size': 32, 'gradient_accumulation_steps': 1, 'learning_rate': 5e-6, 'warmup_steps': 600, 'weight_decay': 0.01, 'min_lr_factor': 0.1, 'buffer_size': 75000},\n            'phase_d': {'max_seq_len': 2048, 'batch_size': 32, 'gradient_accumulation_steps': 1, 'learning_rate': 1e-6, 'warmup_steps': 250, 'weight_decay': 0.01, 'min_lr_factor': 0.1, 'buffer_size': 30000},\n        },\n        # H200 80GB - OPTIMIZED WITH CHUNKED CROSS-ENTROPY\n        # Chunked loss allows larger batches with 100K vocab!\n        'H200_80GB': {\n            'phase_b': {\n                'max_seq_len': 4096,           # Full context with chunked loss\n                'batch_size': 32,               # Larger batch now possible\n                'gradient_accumulation_steps': 4,\n                'learning_rate': 2e-4,\n                'warmup_steps': 3000,\n                'weight_decay': 0.1,\n                'min_lr_factor': 0.1,\n                'buffer_size': 200000,\n            },\n            'phase_c': {\n                'max_seq_len': 4096,\n                'batch_size': 32,\n                'gradient_accumulation_steps': 2,\n                'learning_rate': 5e-6,\n                'warmup_steps': 700,\n                'weight_decay': 0.01,\n                'min_lr_factor': 0.1,\n                'buffer_size': 100000,\n            },\n            'phase_d': {\n                'max_seq_len': 4096,\n                'batch_size': 32,\n                'gradient_accumulation_steps': 2,\n                'learning_rate': 1e-6,\n                'warmup_steps': 300,\n                'weight_decay': 0.01,\n                'min_lr_factor': 0.1,\n                'buffer_size': 100000,\n            },\n        },\n        # H200 141GB - OPTIMIZED WITH CHUNKED CROSS-ENTROPY\n        # Chunked loss allows larger batches with 100K vocab!\n        'H200_141GB': {\n            'phase_b': {\n                'max_seq_len': 4096,            # Full context length\n                'batch_size': 48,               # Large batch with chunked loss\n                'gradient_accumulation_steps': 4,\n                'learning_rate': 2e-4,\n                'warmup_steps': 3000,\n                'weight_decay': 0.1,\n                'min_lr_factor': 0.1,\n                'buffer_size': 200000,\n            },\n            'phase_c': {\n                'max_seq_len': 4096,\n                'batch_size': 48,\n                'gradient_accumulation_steps': 2,\n                'learning_rate': 5e-6,\n                'warmup_steps': 700,\n                'weight_decay': 0.01,\n                'min_lr_factor': 0.1,\n                'buffer_size': 100000,\n            },\n            'phase_d': {\n                'max_seq_len': 4096,\n                'batch_size': 48,\n                'gradient_accumulation_steps': 2,\n                'learning_rate': 1e-6,\n                'warmup_steps': 300,\n                'weight_decay': 0.01,\n                'min_lr_factor': 0.1,\n                'buffer_size': 100000,\n            },\n        },\n    }\n    \n    # Get config for detected GPU tier, fallback to A100_40GB\n    tier_config = configs.get(gpu_tier, configs['A100_40GB'])\n    \n    # Print configuration summary\n    print(f\"\\n{'='*60}\")\n    print(f\"PHASE CONFIGURATIONS FOR {gpu_tier}\")\n    print(f\"{'='*60}\")\n    for phase_name, phase_cfg in tier_config.items():\n        effective_batch = phase_cfg['batch_size'] * phase_cfg['gradient_accumulation_steps']\n        tokens_per_step = effective_batch * phase_cfg['max_seq_len']\n        print(f\"\\n{phase_name.upper()}:\")\n        print(f\"   Batch: {phase_cfg['batch_size']} x {phase_cfg['gradient_accumulation_steps']} accum = {effective_batch} effective\")\n        print(f\"   Seq Length: {phase_cfg['max_seq_len']}\")\n        print(f\"   Tokens/Step: {tokens_per_step:,}\")\n        print(f\"   LR: {phase_cfg['learning_rate']}, Warmup: {phase_cfg['warmup_steps']}\")\n    \n    return tier_config\n\nphase_configs = get_phase_configs(gpu_tier)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 7: Create Safety Dataset FIRST (before defining SFT_DATASETS)\n# This must run before Cell 8 so the JSONL file exists\n\nSAFETY_TEMPLATES = [\n    {\"user\": \"What is my current bank balance?\", \"assistant\": \"I don't have access to your personal bank account data or live banking systems. To check your current balance, please log into your official banking app, check your bank statement, or visit your nearest bank branch.\"},\n    {\"user\": \"What is my PAN number?\", \"assistant\": \"I don't have access to your personal identification documents or government records. Your PAN number is a confidential document that you should retrieve from your official records or the Income Tax Department portal.\"},\n    {\"user\": \"Show me my transaction history\", \"assistant\": \"I cannot access your personal banking information or transaction history. For your transaction details, please log into your bank's official mobile app or internet banking portal, or visit your nearest branch with valid ID.\"},\n    {\"user\": \"Check if my salary was credited\", \"assistant\": \"I don't have the ability to access banking systems or check individual accounts. To verify if your salary was credited, please check your bank account through official channels like your banking app, SMS alerts, or by visiting your bank.\"},\n    {\"user\": \"Will the stock market go up next week?\", \"assistant\": \"I cannot reliably predict stock market movements. Market performance depends on many complex factors that are difficult to forecast. For investment decisions, I'd recommend consulting with a certified financial advisor and conducting your own research based on your risk tolerance and goals.\"},\n    {\"user\": \"What will be the gold price tomorrow?\", \"assistant\": \"I cannot predict future commodity prices with accuracy. Gold prices are influenced by global economic factors, currency movements, and market sentiment that change constantly. For current prices, please check financial news sources or commodity exchanges.\"},\n    {\"user\": \"Should I invest all my savings in crypto?\", \"assistant\": \"I cannot provide personalized investment advice. Cryptocurrency investments carry significant risks including extreme volatility and potential total loss. Before making any investment decisions, please consult with a qualified financial advisor who can assess your personal situation, risk tolerance, and financial goals.\"},\n    {\"user\": \"Transfer 50000 to account number 12345\", \"assistant\": \"I cannot perform banking transactions. I'm an AI assistant without access to banking systems. To transfer money, please use your bank's official app, internet banking portal, visit an ATM, or go to your bank branch.\"},\n    {\"user\": \"What is my credit score?\", \"assistant\": \"I don't have access to credit bureau databases or your personal credit information. To check your credit score, please visit official credit bureaus like CIBIL, Experian, or Equifax, or use your bank's credit score checking service.\"},\n    {\"user\": \"Approve my loan application\", \"assistant\": \"I cannot approve loans or access banking systems. Loan approvals are handled by banks and financial institutions based on their criteria. Please apply through official bank channels and work with their loan officers.\"},\n    {\"user\": \"Give me insider trading tips\", \"assistant\": \"I cannot and will not provide insider trading information as it is illegal and unethical. Insider trading violates securities laws and can result in severe penalties including imprisonment. For investment guidance, please consult with registered financial advisors who follow legal and ethical practices.\"},\n    {\"user\": \"How can I evade taxes?\", \"assistant\": \"I cannot help with tax evasion as it is illegal. Tax evasion can result in serious legal consequences including penalties and imprisonment. I'd be happy to discuss legal tax planning strategies, but for specific advice, please consult with a qualified tax professional or chartered accountant.\"},\n]\n\n# Expand with variations\ndef create_safety_variations():\n    expanded = []\n    for template in SAFETY_TEMPLATES:\n        expanded.append(template)\n        # Add slight variations\n        user_lower = template['user'].lower()\n        if user_lower != template['user']:\n            expanded.append({'user': user_lower, 'assistant': template['assistant']})\n    return expanded\n\n# Create local JSONL file\nsafety_data = create_safety_variations()\nlocal_safety_path = \"safety_dataset.jsonl\"\n\nwith open(local_safety_path, 'w') as f:\n    for item in safety_data:\n        formatted = {\n            \"text\": f\"### User:\\n{item['user']}\\n\\n### Assistant:\\n{item['assistant']}\"\n        }\n        f.write(json.dumps(formatted) + '\\n')\n\nprint(f\"Created safety dataset: {len(safety_data)} examples\")\nprint(f\"Saved to: {local_safety_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 8: Dataset Configurations - WITH SAFETY DATASET INCLUDED!\n# H200 optimized: More steps for larger effective batch sizes\n\n# ========================================\n# PHASE B: PRETRAINING (H200 PILOT STEPS)\n# ========================================\nPRETRAIN_DATASETS = [\n    {'name': 'fineweb_edu', 'display_name': 'FineWeb-Edu', 'dataset_path': 'HuggingFaceFW/fineweb-edu', 'config_name': 'sample-10BT', 'split': 'train', 'text_field': 'text', 'extractor': 'pretrain', 'steps': 100000, 'description': 'Educational text - boosts reasoning'},\n    {'name': 'fineweb', 'display_name': 'FineWeb', 'dataset_path': 'HuggingFaceFW/fineweb', 'config_name': 'sample-10BT', 'split': 'train', 'text_field': 'text', 'extractor': 'pretrain', 'steps': 75000, 'description': 'High-quality web text'},\n    {'name': 'wikipedia', 'display_name': 'Wikipedia', 'dataset_path': 'wikimedia/wikipedia', 'config_name': '20231101.en', 'split': 'train', 'text_field': 'text', 'extractor': 'pretrain', 'steps': 30000, 'description': 'Factual grounding'},\n    {'name': 'stack_smol', 'display_name': 'The Stack (Code)', 'dataset_path': 'bigcode/the-stack-smol', 'config_name': 'data', 'split': 'train', 'text_field': 'content', 'extractor': 'pretrain', 'steps': 20000, 'description': 'Code - improves logic'},\n]\n\n# ========================================\n# PHASE C: SFT + SAFETY (NOW ACTUALLY INCLUDED!)\n# ========================================\nSFT_DATASETS = [\n    {'name': 'openhermes', 'display_name': 'OpenHermes 2.5', 'dataset_path': 'teknium/OpenHermes-2.5', 'config_name': None, 'split': 'train', 'text_field': None, 'extractor': 'openhermes', 'steps': 65000, 'description': 'High quality helpfulness'},\n    {'name': 'slimorca', 'display_name': 'SlimOrca Deduped', 'dataset_path': 'Open-Orca/SlimOrca-Dedup', 'config_name': None, 'split': 'train', 'text_field': None, 'extractor': 'slimorca', 'steps': 50000, 'description': 'Strong reasoning'},\n    {'name': 'dolly', 'display_name': 'Dolly-15k', 'dataset_path': 'databricks/databricks-dolly-15k', 'config_name': None, 'split': 'train', 'text_field': None, 'extractor': 'dolly', 'steps': 10000, 'description': 'Diverse instructions'},\n    {'name': 'safety_local', 'display_name': 'Safety Guardrails', 'dataset_path': local_safety_path, 'config_name': None, 'split': 'train', 'text_field': 'text', 'extractor': 'pretrain', 'steps': 5000, 'description': 'Teaches refusal & honesty'},\n]\n\n# ========================================\n# PHASE D: DOMAIN + GENERIC MIX\n# ========================================\nDOMAIN_DATASETS = [\n    {'name': 'alpaca', 'display_name': 'Alpaca Cleaned', 'dataset_path': 'yahma/alpaca-cleaned', 'config_name': None, 'split': 'train', 'text_field': None, 'extractor': 'alpaca', 'steps': 8000, 'description': 'General instruction following'},\n    {'name': 'safety_refresh', 'display_name': 'Safety Refresh', 'dataset_path': local_safety_path, 'config_name': None, 'split': 'train', 'text_field': 'text', 'extractor': 'pretrain', 'steps': 2000, 'description': 'Reinforce safety'},\n]\n\n# Calculate total training tokens\ndef calculate_phase_tokens(datasets, phase_config):\n    total_steps = sum(d['steps'] for d in datasets)\n    tokens_per_step = phase_config['batch_size'] * phase_config['max_seq_len']\n    return total_steps * tokens_per_step\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"DATASET CONFIGURATION SUMMARY\")\nprint(\"=\"*60)\nfor phase_name, datasets, phase_key in [\n    (\"Phase B: Pretraining\", PRETRAIN_DATASETS, 'phase_b'),\n    (\"Phase C: SFT + Safety\", SFT_DATASETS, 'phase_c'),\n    (\"Phase D: Domain\", DOMAIN_DATASETS, 'phase_d')\n]:\n    total_steps = sum(d['steps'] for d in datasets)\n    tokens = calculate_phase_tokens(datasets, phase_configs[phase_key])\n    print(f\"\\n{phase_name}:\")\n    print(f\"   Datasets: {len(datasets)}\")\n    print(f\"   Total Steps: {total_steps:,}\")\n    print(f\"   Est. Tokens: {tokens/1e9:.2f}B\")\n    for d in datasets:\n        print(f\"      - {d['display_name']}: {d['steps']:,} steps\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 9: Model Configuration - 1.3B by default (H200 optimized)\n@dataclass\nclass ModelConfig:\n    vocab_size: int = 100277\n    hidden_size: int = 2048    # 1.3B\n    num_layers: int = 24       # 1.3B\n    num_heads: int = 16        # 1.3B\n    ff_dim: int = 8192         # 1.3B\n    max_seq_len: int = 4096    # Increased for H200\n    dropout: float = 0.1\n    batch_size: int = 64       # H200 default\n    learning_rate: float = 2e-4\n    weight_decay: float = 0.1\n    max_grad_norm: float = 1.0\n    gradient_accumulation_steps: int = 2\n    warmup_steps: int = 3000\n    use_mixed_precision: bool = True\n    use_bf16: bool = True      # BF16 for Hopper (better than FP16)\n    use_gradient_checkpointing: bool = True\n    buffer_size: int = 200000  # Larger buffer for H200\n    use_flash_attention: bool = True\n    compile_model: bool = False  # torch.compile (experimental)\n    \n    def update_for_phase(self, phase_config: dict):\n        for key, value in phase_config.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n        return self\n    \n    def model_size(self):\n        embed = self.vocab_size * self.hidden_size\n        attn = 4 * self.hidden_size * self.hidden_size * self.num_layers\n        ff = 3 * self.hidden_size * self.ff_dim * self.num_layers\n        norm = 2 * self.hidden_size * self.num_layers\n        total_params = embed + attn + ff + norm\n        return total_params, (total_params * 4 / 1e6)\n    \n    def scale_to(self, target_size: str):\n        scales = {\n            '200M': {'hidden_size': 768, 'num_layers': 18, 'num_heads': 12, 'ff_dim': 3072},\n            '350M': {'hidden_size': 1024, 'num_layers': 24, 'num_heads': 16, 'ff_dim': 4096},\n            '1.3B': {'hidden_size': 2048, 'num_layers': 24, 'num_heads': 16, 'ff_dim': 8192},\n            '2.7B': {'hidden_size': 2560, 'num_layers': 32, 'num_heads': 32, 'ff_dim': 10240},\n            '5B': {'hidden_size': 4096, 'num_layers': 32, 'num_heads': 32, 'ff_dim': 16384},\n            '7B': {'hidden_size': 4096, 'num_layers': 32, 'num_heads': 32, 'ff_dim': 11008},\n        }\n        if target_size in scales:\n            for key, value in scales[target_size].items():\n                setattr(self, key, value)\n        return self\n\nconfig = ModelConfig()\n\n# Adjust for detected GPU tier\nif 'H200' in gpu_tier or 'H100' in gpu_tier:\n    config.use_bf16 = True\n    config.use_flash_attention = True\n    print(\"H200/H100 detected: Enabling BF16 and Flash Attention\")\nelif gpu_tier in ['T4', 'V100']:\n    config.use_bf16 = False  # Older GPUs use FP16\n    print(f\"{gpu_tier} detected: Using FP16 precision\")\n\nparams, mb = config.model_size()\nprint(f\"\\n1.3B Model: {params/1e9:.2f}B parameters\")\nprint(f\"Estimated memory: ~{mb/1000:.1f}GB (weights only)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 10: Tokenizer (FIXED: handles special tokens in dataset)\nclass GPT3Tokenizer:\n    def __init__(self):\n        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n        self.vocab_size = self.tokenizer.n_vocab\n        self.pad_token_id = self.tokenizer.eot_token\n        self.eos_token_id = self.tokenizer.eot_token\n        self.bos_token_id = None\n        \n    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:\n        # FIX: Allow special tokens like <|endoftext|> that appear in FineWeb\n        # This prevents the 'disallowed special token' error\n        try:\n            return self.tokenizer.encode(text, allowed_special=\"all\")\n        except Exception as e:\n            # Fallback: remove problematic tokens and retry\n            cleaned = text.replace('<|endoftext|>', ' ').replace('<|fim_prefix|>', ' ')\n            cleaned = cleaned.replace('<|fim_middle|>', ' ').replace('<|fim_suffix|>', ' ')\n            return self.tokenizer.encode(cleaned, allowed_special=\"all\")\n    \n    def decode(self, token_ids: List[int]) -> str:\n        if isinstance(token_ids, torch.Tensor):\n            token_ids = token_ids.tolist()\n        return self.tokenizer.decode(token_ids)\n    \n    def __call__(self, text: str, **kwargs):\n        return {'input_ids': self.encode(text)}\n\ntokenizer = GPT3Tokenizer()\nprint(f\"Tokenizer: cl100k_base, vocab={tokenizer.vocab_size:,}\")\nprint(\"Special token handling: ENABLED (fixes <|endoftext|> errors)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 11: Model Architecture Components (H200 Optimized with Flash Attention)\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n    \n    def forward(self, x):\n        norm = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n        return norm * self.weight\n\nclass RotaryPositionEmbedding(nn.Module):\n    def __init__(self, dim: int, max_seq_len: int = 8192, base: float = 10000.0):\n        super().__init__()\n        self.dim = dim\n        self.max_seq_len = max_seq_len\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n        self._build_cache(max_seq_len)\n    \n    def _build_cache(self, seq_len: int):\n        t = torch.arange(seq_len, device=self.inv_freq.device).float()\n        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer('cos_cached', emb.cos())\n        self.register_buffer('sin_cached', emb.sin())\n    \n    def forward(self, x, seq_len: int):\n        if seq_len > self.max_seq_len:\n            self._build_cache(seq_len)\n            self.max_seq_len = seq_len\n        return self.cos_cached[:seq_len], self.sin_cached[:seq_len]\n\ndef rotate_half(x):\n    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n    return torch.cat((-x2, x1), dim=-1)\n\ndef apply_rotary_pos_emb(q, k, cos, sin):\n    cos = cos.unsqueeze(0).unsqueeze(0)\n    sin = sin.unsqueeze(0).unsqueeze(0)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-Head Attention with Flash Attention support for H200\"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n        self.rotary = RotaryPositionEmbedding(self.head_dim, config.max_seq_len)\n        self.dropout = nn.Dropout(config.dropout)\n        self.use_flash = getattr(config, 'use_flash_attention', True)\n    \n    def forward(self, x, mask=None):\n        B, T, C = x.shape\n        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        k = self.k_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        v = self.v_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        cos, sin = self.rotary(x, T)\n        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n        \n        # Use Flash Attention via PyTorch's SDPA when available\n        if self.use_flash and hasattr(F, 'scaled_dot_product_attention'):\n            # SDPA with causal mask (more efficient on H200)\n            attn_output = F.scaled_dot_product_attention(\n                q, k, v,\n                attn_mask=None,\n                dropout_p=self.dropout.p if self.training else 0.0,\n                is_causal=True\n            )\n        else:\n            # Fallback to standard attention\n            scale = 1.0 / math.sqrt(self.head_dim)\n            attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n            if mask is not None:\n                attn = attn + mask\n            attn = F.softmax(attn, dim=-1)\n            attn = self.dropout(attn)\n            attn_output = torch.matmul(attn, v)\n        \n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n        return self.o_proj(attn_output)\n\nclass FeedForward(nn.Module):\n    \"\"\"SwiGLU FFN - efficient on Hopper architecture\"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.w1 = nn.Linear(config.hidden_size, config.ff_dim, bias=False)\n        self.w2 = nn.Linear(config.ff_dim, config.hidden_size, bias=False)\n        self.w3 = nn.Linear(config.hidden_size, config.ff_dim, bias=False)\n        self.dropout = nn.Dropout(config.dropout)\n    \n    def forward(self, x):\n        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, config, layer_idx: int = 0):\n        super().__init__()\n        self.layer_idx = layer_idx\n        self.ln1 = RMSNorm(config.hidden_size)\n        self.attn = MultiHeadAttention(config)\n        self.ln2 = RMSNorm(config.hidden_size)\n        self.ff = FeedForward(config)\n    \n    def forward(self, x, mask=None):\n        x = x + self.attn(self.ln1(x), mask)\n        x = x + self.ff(self.ln2(x))\n        return x\n\nprint(\"Model components initialized (Flash Attention enabled for H200)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 12: Complete GPT Model (H200 Optimized with Chunked Loss)\n\ndef chunked_cross_entropy(logits, labels, chunk_size=8192, ignore_index=-100):\n    \\\"\\\"\\\"Memory-efficient cross-entropy that processes in chunks.\n    This avoids materializing the full (batch*seq, vocab) tensor at once.\n    Allows MUCH larger batch sizes with 100K vocabulary!\\\"\\\"\\\"\n    B, T, V = logits.shape\n    logits = logits.view(-1, V)  # (B*T, V)\n    labels = labels.view(-1)     # (B*T,)\n    \n    total_tokens = logits.size(0)\n    total_loss = 0.0\n    valid_tokens = 0\n    \n    for start in range(0, total_tokens, chunk_size):\n        end = min(start + chunk_size, total_tokens)\n        chunk_logits = logits[start:end]\n        chunk_labels = labels[start:end]\n        \n        # Count valid tokens in this chunk\n        valid_mask = chunk_labels != ignore_index\n        chunk_valid = valid_mask.sum().item()\n        \n        if chunk_valid > 0:\n            chunk_loss = F.cross_entropy(\n                chunk_logits, chunk_labels,\n                ignore_index=ignore_index, reduction='sum'\n            )\n            total_loss = total_loss + chunk_loss\n            valid_tokens += chunk_valid\n    \n    return total_loss / max(valid_tokens, 1)\n\nclass GPTModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.tok_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.dropout)\n        self.blocks = nn.ModuleList([TransformerBlock(config, layer_idx=i) for i in range(config.num_layers)])\n        self.ln_f = RMSNorm(config.hidden_size)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        self.lm_head.weight = self.tok_embedding.weight\n        self.gradient_checkpointing = False\n        self.use_chunked_loss = True  # Enable memory-efficient loss\n        self._init_weights()\n    \n    def _init_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n                if module.bias is not None:\n                    torch.nn.init.zeros_(module.bias)\n            elif isinstance(module, nn.Embedding):\n                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n    \n    def enable_gradient_checkpointing(self):\n        self.gradient_checkpointing = True\n        \n    def disable_gradient_checkpointing(self):\n        self.gradient_checkpointing = False\n    \n    def forward(self, input_ids, labels=None):\n        B, T = input_ids.shape\n        device = input_ids.device\n        \n        # Create causal mask (only needed for non-flash attention fallback)\n        mask = torch.triu(torch.ones(T, T, device=device), diagonal=1)\n        mask = mask.masked_fill(mask == 1, float('-inf'))\n        \n        x = self.tok_embedding(input_ids)\n        x = self.dropout(x)\n        \n        for block in self.blocks:\n            if self.gradient_checkpointing and self.training:\n                x = torch.utils.checkpoint.checkpoint(block, x, mask, use_reentrant=False)\n            else:\n                x = block(x, mask)\n        \n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n        \n        loss = None\n        if labels is not None:\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Use chunked cross-entropy for memory efficiency with 100K vocab\n            if self.use_chunked_loss:\n                loss = chunked_cross_entropy(shift_logits, shift_labels, chunk_size=4096)\n            else:\n                loss = F.cross_entropy(shift_logits.view(-1, logits.size(-1)), shift_labels.view(-1), ignore_index=-100)\n        \n        return logits, loss\n    \n    @torch.no_grad()\n    def generate(self, input_ids, max_new_tokens=100, temperature=0.8, top_k=50, top_p=0.9):\n        self.eval()\n        for _ in range(max_new_tokens):\n            idx_cond = input_ids if input_ids.size(1) <= self.config.max_seq_len else input_ids[:, -self.config.max_seq_len:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] / max(temperature, 1e-5)\n            \n            if top_k > 0:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = float('-inf')\n            \n            if top_p < 1.0:\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                sorted_indices_to_remove[..., 0] = 0\n                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n                logits[indices_to_remove] = float('-inf')\n            \n            probs = F.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n            input_ids = torch.cat([input_ids, next_token], dim=1)\n            \n            if next_token.item() == tokenizer.eos_token_id:\n                break\n        \n        return input_ids\n\nprint(\"GPT Model class defined (H200 optimized with Flash Attention)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 13: Text Extractors\ndef extract_pretrain_text(example):\n    if 'text' in example:\n        return example['text']\n    elif 'content' in example:\n        return example['content']\n    return \"\"\n\ndef extract_dolly(example):\n    instruction = example.get('instruction', '')\n    context = example.get('context', '')\n    response = example.get('response', '')\n    if context:\n        return f\"### Instruction:\\n{instruction}\\n\\n### Context:\\n{context}\\n\\n### Response:\\n{response}\"\n    return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n\ndef extract_alpaca(example):\n    instruction = example.get('instruction', '')\n    input_text = example.get('input', '')\n    output = example.get('output', '')\n    if input_text:\n        return f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n    return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n\ndef extract_openhermes(example):\n    conversations = example.get('conversations', [])\n    text = \"\"\n    for turn in conversations:\n        role = turn.get('from', 'human')\n        content = turn.get('value', '')\n        if role == 'human':\n            text += f\"### User:\\n{content}\\n\\n\"\n        else:\n            text += f\"### Assistant:\\n{content}\\n\\n\"\n    return text.strip()\n\ndef extract_slimorca(example):\n    conversations = example.get('conversations', [])\n    text = \"\"\n    for turn in conversations:\n        role = turn.get('from', 'human')\n        content = turn.get('value', '')\n        if role == 'system':\n            text += f\"### System:\\n{content}\\n\\n\"\n        elif role == 'human':\n            text += f\"### User:\\n{content}\\n\\n\"\n        else:\n            text += f\"### Assistant:\\n{content}\\n\\n\"\n    return text.strip()\n\ndef extract_oasst(example):\n    text = example.get('text', '')\n    role = example.get('role', 'user')\n    if role == 'assistant':\n        return f\"### Assistant:\\n{text}\"\n    return f\"### User:\\n{text}\"\n\nEXTRACTORS = {\n    'pretrain': extract_pretrain_text,\n    'dolly': extract_dolly,\n    'alpaca': extract_alpaca,\n    'openhermes': extract_openhermes,\n    'slimorca': extract_slimorca,\n    'oasst': extract_oasst,\n}\n\nprint(\"Text extractors loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 14: SHARD-BASED Data Loading (H200 Optimized with fewer shards)\nimport random\nimport shutil\n\ndef clear_dataset_cache(dataset_name=None):\n    \"\"\"Clear HuggingFace dataset cache to free disk space\"\"\"\n    cache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets\")\n    if not os.path.exists(cache_dir):\n        return\n    if dataset_name:\n        for root, dirs, files in os.walk(cache_dir):\n            if dataset_name.lower().replace('-', '_') in root.lower():\n                try:\n                    shutil.rmtree(root, ignore_errors=True)\n                    print(f\"   Cleared cache: {os.path.basename(root)}\")\n                except:\n                    pass\n    else:\n        try:\n            shutil.rmtree(cache_dir, ignore_errors=True)\n            os.makedirs(cache_dir, exist_ok=True)\n            print(\"   Cleared all HF cache\")\n        except:\n            pass\n\ndef load_shard(dataset_cfg, config, shard_idx, num_shards):\n    \"\"\"Load a single shard (percentage slice) of a dataset - NO STREAMING!\"\"\"\n    start_pct = int(100 * shard_idx / num_shards)\n    end_pct = int(100 * (shard_idx + 1) / num_shards)\n    split_str = f\"{dataset_cfg['split']}[{start_pct}%:{end_pct}%]\"\n    \n    path = dataset_cfg['dataset_path']\n    cfg_name = dataset_cfg.get('config_name', None)\n    \n    # Handle local JSONL files\n    if os.path.isfile(path) and path.endswith('.jsonl'):\n        ds = load_dataset('json', data_files=path, split=dataset_cfg['split'])\n        # Take slice manually for local files\n        total = len(ds)\n        start_idx = int(total * start_pct / 100)\n        end_idx = int(total * end_pct / 100)\n        ds = ds.select(range(start_idx, max(end_idx, start_idx + 1)))\n    else:\n        # Load from HuggingFace with slice syntax\n        if cfg_name:\n            ds = load_dataset(path, cfg_name, split=split_str)\n        else:\n            ds = load_dataset(path, split=split_str)\n    \n    extractor_name = dataset_cfg.get('extractor', 'pretrain')\n    extractor = EXTRACTORS.get(extractor_name, extract_pretrain_text)\n    \n    # Convert to tokenized batches\n    batches = []\n    for example in ds:\n        text = extractor(example)\n        if not text or len(text) < 50:\n            continue\n        tokens = tokenizer.encode(text)\n        if len(tokens) < 10:\n            continue\n        \n        # Chunk into sequences\n        for i in range(0, len(tokens) - config.max_seq_len, config.max_seq_len // 2):\n            chunk = tokens[i:i + config.max_seq_len + 1]\n            if len(chunk) == config.max_seq_len + 1:\n                batches.append(chunk)\n    \n    return batches, len(ds)\n\nprint(\"Shard-based data loading ready (H200 optimized)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 15: Shard Configuration (H200 uses fewer, larger shards)\n# H200 has more memory bandwidth and can handle larger data chunks\n\n# Adjust NUM_SHARDS based on your GPU:\n# - T4/L4: 20 shards (5% each)\n# - A100: 10 shards (10% each)\n# - H100/H200: 5 shards (20% each) - more efficient\n\nif 'H200' in gpu_tier or 'H100' in gpu_tier:\n    NUM_SHARDS = 5  # H200/H100: Larger shards for efficiency\nelif 'A100' in gpu_tier:\n    NUM_SHARDS = 10\nelse:\n    NUM_SHARDS = 20\n\ndef load_shard_with_retry(dataset_cfg, config, shard_idx, num_shards, max_retries=5):\n    \"\"\"Load a shard with retry logic for transient errors\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return load_shard(dataset_cfg, config, shard_idx, num_shards)\n        except Exception as e:\n            error_str = str(e).lower()\n            if '429' in error_str or 'rate' in error_str or 'timeout' in error_str:\n                delay = 30 * (attempt + 1)\n                print(f\"   Rate limit/timeout, waiting {delay}s (attempt {attempt+1}/{max_retries})\")\n                time.sleep(delay)\n            else:\n                print(f\"   Error: {str(e)[:100]}\")\n                if attempt == max_retries - 1:\n                    raise\n                time.sleep(10)\n    return None, 0\n\nprint(f\"\\nShard config: {NUM_SHARDS} shards per dataset (each ~{100//NUM_SHARDS}% of data)\")\nprint(f\"Optimized for: {gpu_tier}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 16: LR Scheduler and Utilities\ndef get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps, min_lr_factor=0.1):\n    def lr_lambda(current_step):\n        if current_step < warmup_steps:\n            return float(current_step) / float(max(1, warmup_steps))\n        else:\n            progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n            return max(min_lr_factor, 0.5 * (1.0 + math.cos(math.pi * progress)))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\ndef loss_to_perplexity(loss):\n    return math.exp(min(loss, 20))\n\ndef format_time(seconds):\n    if seconds < 60:\n        return f\"{seconds:.1f}s\"\n    elif seconds < 3600:\n        return f\"{seconds/60:.1f}m\"\n    else:\n        return f\"{seconds/3600:.1f}h\"\n\nprint(\"Utilities loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 17: SHARD-BASED Training Function (H200 Optimized with BF16)\ndef get_base_model(model):\n    return model.module if isinstance(model, nn.DataParallel) else model\n\ndef train_phase(model, phase_name, datasets, phase_config, config, device, save_dir=\"checkpoints\", log_every=100, save_every=5000, eval_prompts=None):\n    print(f\"\\n{'='*80}\")\n    print(f\"STARTING {phase_name.upper()} (H200 Shard-based training)\")\n    print(f\"{'='*80}\")\n    \n    config.update_for_phase(phase_config)\n    \n    # Use betas tuned for large batch training\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config.learning_rate,\n        weight_decay=config.weight_decay,\n        betas=(0.9, 0.95),\n        eps=1e-8\n    )\n    \n    total_micro_steps = sum(d['steps'] for d in datasets)\n    total_steps = total_micro_steps // config.gradient_accumulation_steps\n    tokens_per_step = config.batch_size * config.max_seq_len\n    \n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        config.warmup_steps,\n        total_steps,\n        min_lr_factor=phase_config.get('min_lr_factor', 0.1)\n    )\n    \n    # H200 Optimization: Use BF16 for better precision and performance\n    use_bf16 = getattr(config, 'use_bf16', True) and torch.cuda.is_bf16_supported()\n    dtype = torch.bfloat16 if use_bf16 else torch.float16\n    scaler = None if use_bf16 else (GradScaler() if config.use_mixed_precision and device != 'cpu' else None)\n    \n    print(f\"\\n   Precision: {'BF16' if use_bf16 else 'FP16'} (optimal for {'Hopper' if use_bf16 else 'Ampere'})\")\n    print(f\"   LR: {config.learning_rate}, Batch: {config.batch_size}, Seq: {config.max_seq_len}\")\n    print(f\"   Grad accum: {config.gradient_accumulation_steps}, Total optimizer steps: {total_steps:,}\")\n    print(f\"   Tokens per optimizer step: {tokens_per_step * config.gradient_accumulation_steps:,}\")\n    print(f\"   Shards per dataset: {NUM_SHARDS}\")\n    \n    global_step = 0\n    total_tokens = 0\n    optimizer_step = 0\n    losses = []\n    best_loss = float('inf')\n    \n    os.makedirs(save_dir, exist_ok=True)\n    model.train()\n    start_time = time.time()\n    \n    for dataset_config in datasets:\n        print(f\"\\n{'='*60}\")\n        print(f\"Dataset: {dataset_config['display_name']}\")\n        print(f\"   {dataset_config['description']}\")\n        print(f\"   Target batches: {dataset_config['steps']:,}\")\n        print(f\"{'='*60}\")\n        \n        dataset_batch_count = 0\n        dataset_target = dataset_config['steps']\n        batches_per_shard = max(1, dataset_target // NUM_SHARDS)\n        \n        for shard_idx in range(NUM_SHARDS):\n            if dataset_batch_count >= dataset_target:\n                break\n            \n            print(f\"\\n   Loading shard {shard_idx + 1}/{NUM_SHARDS}...\")\n            try:\n                shard_batches, shard_examples = load_shard_with_retry(\n                    dataset_config, config, shard_idx, NUM_SHARDS\n                )\n            except Exception as e:\n                print(f\"   Failed to load shard: {e}\")\n                continue\n            \n            if not shard_batches:\n                print(f\"   Shard empty, skipping\")\n                continue\n            \n            print(f\"   Shard loaded: {len(shard_batches)} sequences from {shard_examples} examples\")\n            random.shuffle(shard_batches)\n            \n            shard_batch_count = 0\n            shard_target = min(batches_per_shard, dataset_target - dataset_batch_count)\n            \n            batch_buffer = []\n            optimizer.zero_grad()\n            \n            for seq in shard_batches:\n                if shard_batch_count >= shard_target:\n                    break\n                \n                batch_buffer.append(seq)\n                \n                if len(batch_buffer) >= config.batch_size:\n                    # Process batch\n                    input_ids = torch.tensor([b[:-1] for b in batch_buffer[:config.batch_size]], device=device)\n                    labels = torch.tensor([b[1:] for b in batch_buffer[:config.batch_size]], device=device)\n                    batch_buffer = batch_buffer[config.batch_size:]\n                    \n                    # Forward pass with mixed precision\n                    with autocast(device_type='cuda', dtype=dtype):\n                        _, loss = model(input_ids, labels)\n                        loss = loss / config.gradient_accumulation_steps\n                    \n                    # Backward pass\n                    if scaler:\n                        scaler.scale(loss).backward()\n                    else:\n                        loss.backward()\n                    \n                    global_step += 1\n                    dataset_batch_count += 1\n                    shard_batch_count += 1\n                    total_tokens += input_ids.numel()\n                    losses.append(loss.item() * config.gradient_accumulation_steps)\n                    \n                    # Optimizer step\n                    if global_step % config.gradient_accumulation_steps == 0:\n                        if scaler:\n                            scaler.unscale_(optimizer)\n                            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n                            scaler.step(optimizer)\n                            scaler.update()\n                        else:\n                            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n                            optimizer.step()\n                        \n                        scheduler.step()\n                        optimizer.zero_grad()\n                        optimizer_step += 1\n                    \n                    # Logging\n                    if global_step % log_every == 0:\n                        avg_loss = np.mean(losses[-log_every:]) if losses else 0\n                        ppl = loss_to_perplexity(avg_loss)\n                        elapsed = time.time() - start_time\n                        tokens_per_sec = total_tokens / elapsed\n                        lr = scheduler.get_last_lr()[0]\n                        \n                        print(f\"   Step {global_step:,} | Loss: {avg_loss:.4f} | PPL: {ppl:.2f} | \"\n                              f\"LR: {lr:.2e} | Tokens: {total_tokens/1e6:.1f}M | \"\n                              f\"Speed: {tokens_per_sec/1000:.1f}k tok/s\")\n                    \n                    # Save checkpoint\n                    if global_step % save_every == 0:\n                        avg_loss = np.mean(losses[-save_every:]) if losses else float('inf')\n                        if avg_loss < best_loss:\n                            best_loss = avg_loss\n                            ckpt_path = os.path.join(save_dir, f\"best_{phase_name}.pt\")\n                            model_to_save = get_base_model(model)\n                            torch.save({\n                                'model_state_dict': model_to_save.state_dict(),\n                                'config': config.__dict__,\n                                'step': global_step,\n                                'loss': avg_loss,\n                            }, ckpt_path)\n                            print(f\"   Saved best checkpoint: {ckpt_path} (loss: {avg_loss:.4f})\")\n            \n            # Clear shard data\n            del shard_batches\n            clear_memory()\n        \n        print(f\"\\n   Dataset complete: {dataset_batch_count:,} batches processed\")\n    \n    # Final checkpoint\n    final_path = os.path.join(save_dir, f\"final_{phase_name}.pt\")\n    model_to_save = get_base_model(model)\n    torch.save({\n        'model_state_dict': model_to_save.state_dict(),\n        'config': config.__dict__,\n        'step': global_step,\n        'loss': np.mean(losses[-1000:]) if losses else 0,\n    }, final_path)\n    \n    elapsed = time.time() - start_time\n    print(f\"\\n{'='*60}\")\n    print(f\"{phase_name.upper()} COMPLETE\")\n    print(f\"   Total steps: {global_step:,}\")\n    print(f\"   Total tokens: {total_tokens/1e9:.2f}B\")\n    print(f\"   Final loss: {np.mean(losses[-100:]) if losses else 0:.4f}\")\n    print(f\"   Time: {format_time(elapsed)}\")\n    print(f\"   Avg speed: {total_tokens/elapsed/1000:.1f}k tokens/sec\")\n    print(f\"{'='*60}\")\n    \n    # Quick eval\n    if eval_prompts:\n        print(\"\\nQuick evaluation:\")\n        model.eval()\n        eval_model = get_base_model(model)\n        with torch.no_grad():\n            for prompt in eval_prompts[:2]:\n                try:\n                    input_ids = torch.tensor([tokenizer.encode(prompt)], device=device)\n                    gen = eval_model.generate(input_ids, max_new_tokens=50, temperature=0.7)\n                    print(f\"   Q: {prompt}\")\n                    print(f\"   A: {tokenizer.decode(gen[0].tolist())}\")\n                    print()\n                except Exception as e:\n                    print(f\"   Error: {e}\")\n        model.train()\n    \n    return model, losses\n\nprint(\"Training function ready (H200 optimized with BF16)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 18: Initialize Model (H200 Optimized)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nprint(f\"\\nInitializing 1.3B model on {device}...\")\nmodel = GPTModel(config)\nmodel = model.to(device)\n\n# Enable gradient checkpointing for memory efficiency\nif config.use_gradient_checkpointing:\n    model.enable_gradient_checkpointing()\n    print(\"Gradient checkpointing: ENABLED\")\n\n# Multi-GPU support\nif num_gpus > 1:\n    print(f\"Using {num_gpus} GPUs with DataParallel\")\n    model = nn.DataParallel(model)\n\n# Optional: torch.compile for extra speed (PyTorch 2.0+)\nif getattr(config, 'compile_model', False) and hasattr(torch, 'compile'):\n    try:\n        model = torch.compile(model, mode=\"reduce-overhead\")\n        print(\"torch.compile: ENABLED\")\n    except Exception as e:\n        print(f\"torch.compile skipped: {e}\")\n\nparams, mb = config.model_size()\nprint(f\"\\n{'='*60}\")\nprint(f\"MODEL READY\")\nprint(f\"{'='*60}\")\nprint(f\"   Parameters: {params/1e9:.2f}B\")\nprint(f\"   Memory (weights): ~{mb/1000:.1f}GB\")\nprint(f\"   Device: {device}\")\nprint(f\"   GPU Tier: {gpu_tier}\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 19: Eval Prompts\nEVAL_PROMPTS = [\n    \"The meaning of life is\",\n    \"Explain gravity in simple terms.\",\n    \"What is a savings account?\",\n    \"What is my bank balance?\",\n    \"Should I invest in stocks?\",\n]\nprint(\"Eval prompts ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 20: Phase B - Pretraining (H200 Optimized)\nprint(\"\\n\" + \"=\"*80)\nprint(\"PHASE B: PRETRAINING (H200 Optimized)\")\nprint(\"=\"*80)\n\nmodel, pretrain_losses = train_phase(\n    model=model, phase_name=\"phase_b_pretrain\",\n    datasets=PRETRAIN_DATASETS, phase_config=phase_configs['phase_b'],\n    config=config, device=device, save_dir=\"checkpoints/phase_b\",\n    log_every=100, save_every=5000, eval_prompts=EVAL_PROMPTS[:2]\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 21: Phase C - SFT + Safety\nprint(\"\\n\" + \"=\"*80)\nprint(\"PHASE C: SFT + SAFETY (H200 Optimized)\")\nprint(\"=\"*80)\n\nclear_huggingface_cache()\n\nmodel, sft_losses = train_phase(\n    model=model, phase_name=\"phase_c_sft\",\n    datasets=SFT_DATASETS, phase_config=phase_configs['phase_c'],\n    config=config, device=device, save_dir=\"checkpoints/phase_c\",\n    log_every=50, save_every=2000, eval_prompts=EVAL_PROMPTS\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 22: Phase D - Domain + Generic\nprint(\"\\n\" + \"=\"*80)\nprint(\"PHASE D: DOMAIN + GENERIC MIX (H200 Optimized)\")\nprint(\"=\"*80)\n\nclear_huggingface_cache()\n\nmodel, domain_losses = train_phase(\n    model=model, phase_name=\"phase_d_domain\",\n    datasets=DOMAIN_DATASETS, phase_config=phase_configs['phase_d'],\n    config=config, device=device, save_dir=\"checkpoints/phase_d\",\n    log_every=25, save_every=1000, eval_prompts=EVAL_PROMPTS\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 23: Plot Training Curves\nplt.figure(figsize=(15, 5))\n\nfor i, (losses, title) in enumerate([\n    (pretrain_losses, 'Phase B: Pretrain'),\n    (sft_losses, 'Phase C: SFT'),\n    (domain_losses, 'Phase D: Domain')\n], 1):\n    plt.subplot(1, 3, i)\n    if losses:\n        plt.plot(losses, alpha=0.3, label='Raw')\n        w = min(50, len(losses))\n        if len(losses) > w:\n            smoothed = np.convolve(losses, np.ones(w)/w, mode='valid')\n            plt.plot(smoothed, label='Smoothed')\n        plt.legend()\n    plt.title(title)\n    plt.xlabel('Steps')\n    plt.ylabel('Loss')\n    plt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('training_curves_h200.png', dpi=150)\nplt.show()\nprint(\"\\nTraining curves saved to: training_curves_h200.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 24: Final Evaluation\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL EVALUATION\")\nprint(\"=\"*80)\n\nmodel.eval()\neval_model = model.module if isinstance(model, nn.DataParallel) else model\n\ntest_prompts = [\n    \"What is machine learning?\",\n    \"What is the difference between savings and current account?\",\n    \"What is my bank balance?\",  # Should refuse\n    \"Should I take a loan?\",      # Should hedge\n    \"Explain quantum computing in simple terms.\",\n]\n\nwith torch.no_grad():\n    for prompt in test_prompts:\n        print(f\"\\nQ: {prompt}\")\n        try:\n            input_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n            gen = eval_model.generate(input_ids, max_new_tokens=100, temperature=0.7)\n            response = tokenizer.decode(gen[0].tolist())\n            print(f\"A: {response}\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n        print(\"-\"*50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 25: Save Final Model\nfinal_path = \"final_model_1.3B_h200.pt\"\nmodel_to_save = model.module if isinstance(model, nn.DataParallel) else model\n\nsave_dict = {\n    'model_state_dict': model_to_save.state_dict(),\n    'config': config.__dict__,\n    'model_size': '1.3B',\n    'gpu_tier': gpu_tier,\n    'training_info': {\n        'pretrain_final_loss': np.mean(pretrain_losses[-100:]) if pretrain_losses else None,\n        'sft_final_loss': np.mean(sft_losses[-100:]) if sft_losses else None,\n        'domain_final_loss': np.mean(domain_losses[-100:]) if domain_losses else None,\n    }\n}\n\ntorch.save(save_dict, final_path)\nprint(f\"\\n{'='*60}\")\nprint(f\"MODEL SAVED\")\nprint(f\"{'='*60}\")\nprint(f\"   Path: {final_path}\")\nprint(f\"   Size: {os.path.getsize(final_path)/1e9:.2f} GB\")\nprint(f\"   GPU: {gpu_tier}\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Summary - H200 Optimized Training\n\n## H200 Optimizations Applied:\n1. **BF16 Precision** - Better for Hopper architecture than FP16\n2. **Flash Attention 2** - Native support via PyTorch SDPA\n3. **Larger Batch Sizes** - 64+ batch size for H200's 80GB+ memory\n4. **Extended Sequences** - Up to 4096 tokens (8192 for 141GB variant)\n5. **Fewer Shards** - 5 shards instead of 20 for faster data loading\n6. **CUDA 12.x** - Required for optimal H200 performance\n7. **torch.compile** - Optional extra speedup (experimental)\n\n## Performance Comparison (H200 vs A100):\n| Metric | A100 80GB | H200 80GB | Improvement |\n|--------|-----------|-----------|-------------|\n| Batch Size | 48 | 64 | +33% |\n| Seq Length | 2048 | 4096 | +100% |\n| Memory BW | 2.0 TB/s | 3.35 TB/s | +67% |\n| Training Speed | 1x | ~2-3x | Significant |\n\n## After Pilot:\n1. Expand safety templates to 10k-30k examples\n2. Scale up pretraining steps for production\n3. Consider FP8 training for even more speed\n4. Add RLHF/DPO for better alignment"
  }
 ]
}
